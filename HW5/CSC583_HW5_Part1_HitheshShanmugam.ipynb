{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC_583\n",
    "# Hithesh Shanmugam\n",
    "# HW 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'example' in 100d: [-0.12617    0.61724    0.22581    0.39868    0.16111    0.1523\n",
      " -0.14715   -0.29447   -0.27348   -0.13753   -0.20898   -0.73436\n",
      "  0.14144    0.15048    0.09179    0.018613   0.22539    0.15979\n",
      " -0.16935    0.42716    0.042284  -0.3477    -0.11413    0.12222\n",
      " -0.025027  -0.20805   -0.067264  -0.2956    -0.30807   -0.32903\n",
      "  0.19059    0.77141   -0.19332   -0.31069    0.26745    0.32231\n",
      "  0.2065     0.10497    0.49425   -0.38322   -0.12802   -0.069906\n",
      " -0.14828    0.085369  -0.18141    0.14688    0.60968   -0.21131\n",
      " -0.29148   -0.52773    0.59508    0.017369   0.15342    0.81925\n",
      " -0.20643   -2.0378    -0.11884   -0.16826    1.5288     0.15756\n",
      " -0.4994     0.39305    0.12672   -0.10968    1.3671    -0.21006\n",
      "  0.15684    0.0063801  0.43836   -0.18765   -0.29088    0.18619\n",
      "  0.085402   0.13985    0.40794   -0.14811    0.26702   -0.19142\n",
      " -0.6189     0.0091217  0.34971   -0.24079   -0.52476   -0.25071\n",
      " -1.5681     0.22101    0.046796  -0.62616   -0.043358  -0.42865\n",
      " -0.0057843 -0.22611    0.074171   0.091597  -0.40751   -0.08359\n",
      " -0.48413   -1.0718     0.52827    0.058813 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def load_glove_embeddings(file_path, dimension):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector[:dimension]\n",
    "    return embeddings\n",
    "\n",
    "def load_glove_embeddings_by_dimension(directory_path, dimensions):\n",
    "    embeddings = {}\n",
    "    for dimension in dimensions:\n",
    "        file_path = f\"{directory_path}/glove.6B.{dimension}d.txt\"\n",
    "        embeddings[dimension] = load_glove_embeddings(file_path, dimension)\n",
    "    return embeddings\n",
    "\n",
    "glove_directory = 'C:/Users/sures/OneDrive - DePaul University/Desktop/glove.6B'\n",
    "desired_dimensions = [50, 100, 200, 300]\n",
    "\n",
    "# Load the GloVe embeddings for each desired dimension\n",
    "glove_embeddings = load_glove_embeddings_by_dimension(glove_directory, desired_dimensions)\n",
    "\n",
    "# Example usage\n",
    "word = 'example'\n",
    "desired_dimension = 100\n",
    "embedding = glove_embeddings[desired_dimension].get(word)\n",
    "if embedding is not None:\n",
    "    print(f\"Embedding for '{word}' in {desired_dimension}d: {embedding}\")\n",
    "else:\n",
    "    print(f\"No embedding found for '{word}' in {desired_dimension}d.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Vector Semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 50\n",
      "\n",
      "The most similar word to 'dog' is 'cat' with a cosine similarity of 0.9218.\n",
      "The most similar word to 'whale' is 'shark' with a cosine similarity of 0.8336.\n",
      "The most similar word to 'before' is 'after' with a cosine similarity of 0.9512.\n",
      "The most similar word to 'however' is 'although' with a cosine similarity of 0.9801.\n",
      "The most similar word to 'fabricate' is 'fabricating' with a cosine similarity of 0.7595.\n",
      "\n",
      "Dimension: 100\n",
      "\n",
      "The most similar word to 'dog' is 'cat' with a cosine similarity of 0.8798.\n",
      "The most similar word to 'whale' is 'shark' with a cosine similarity of 0.7840.\n",
      "The most similar word to 'before' is 'after' with a cosine similarity of 0.9246.\n",
      "The most similar word to 'however' is 'although' with a cosine similarity of 0.9658.\n",
      "The most similar word to 'fabricate' is 'invent' with a cosine similarity of 0.7040.\n",
      "\n",
      "Dimension: 200\n",
      "\n",
      "The most similar word to 'dog' is 'cat' with a cosine similarity of 0.7445.\n",
      "The most similar word to 'whale' is 'humpback' with a cosine similarity of 0.7029.\n",
      "The most similar word to 'before' is 'after' with a cosine similarity of 0.8931.\n",
      "The most similar word to 'however' is 'although' with a cosine similarity of 0.9336.\n",
      "The most similar word to 'fabricate' is 'fabricating' with a cosine similarity of 0.6184.\n",
      "\n",
      "Dimension: 300\n",
      "\n",
      "The most similar word to 'dog' is 'cat' with a cosine similarity of 0.6817.\n",
      "The most similar word to 'whale' is 'humpback' with a cosine similarity of 0.6860.\n",
      "The most similar word to 'before' is 'after' with a cosine similarity of 0.8256.\n",
      "The most similar word to 'however' is 'although' with a cosine similarity of 0.9071.\n",
      "The most similar word to 'fabricate' is 'fabricating' with a cosine similarity of 0.5648.\n",
      "\n",
      "Dimension: 50\n",
      "\n",
      "dog : puppy :: cat : ?\n",
      "Candidate: puppy - Similarity: 0.9115\n",
      "Candidate: puppies - Similarity: 0.7629\n",
      "Candidate: cat - Similarity: 0.7486\n",
      "\n",
      "speak : speaker :: sing : ?\n",
      "Candidate: speaker - Similarity: 0.7262\n",
      "Candidate: sing - Similarity: 0.6756\n",
      "Candidate: sang - Similarity: 0.6226\n",
      "\n",
      "France : French :: England : ?\n",
      "Candidate: england - Similarity: 0.8859\n",
      "Candidate: scottish - Similarity: 0.8679\n",
      "Candidate: english - Similarity: 0.8374\n",
      "\n",
      "France : wine :: England : ?\n",
      "Candidate: wine - Similarity: 0.6631\n",
      "Candidate: orchard - Similarity: 0.6624\n",
      "Candidate: tasting - Similarity: 0.6325\n",
      "\n",
      "\n",
      "Dimension: 100\n",
      "\n",
      "dog : puppy :: cat : ?\n",
      "Candidate: puppy - Similarity: 0.8448\n",
      "Candidate: cat - Similarity: 0.6821\n",
      "Candidate: kitten - Similarity: 0.6618\n",
      "\n",
      "speak : speaker :: sing : ?\n",
      "Candidate: speaker - Similarity: 0.6911\n",
      "Candidate: sing - Similarity: 0.6133\n",
      "Candidate: whip - Similarity: 0.5220\n",
      "\n",
      "France : French :: England : ?\n",
      "Candidate: england - Similarity: 0.7876\n",
      "Candidate: english - Similarity: 0.7789\n",
      "Candidate: scottish - Similarity: 0.7717\n",
      "\n",
      "France : wine :: England : ?\n",
      "Candidate: wine - Similarity: 0.6935\n",
      "Candidate: tea - Similarity: 0.6067\n",
      "Candidate: wines - Similarity: 0.5992\n",
      "\n",
      "\n",
      "Dimension: 200\n",
      "\n",
      "dog : puppy :: cat : ?\n",
      "Candidate: puppy - Similarity: 0.7711\n",
      "Candidate: cat - Similarity: 0.6286\n",
      "Candidate: puppies - Similarity: 0.6081\n",
      "\n",
      "speak : speaker :: sing : ?\n",
      "Candidate: speaker - Similarity: 0.6879\n",
      "Candidate: sing - Similarity: 0.5925\n",
      "Candidate: sang - Similarity: 0.5009\n",
      "\n",
      "France : French :: England : ?\n",
      "Candidate: english - Similarity: 0.7609\n",
      "Candidate: england - Similarity: 0.7598\n",
      "Candidate: british - Similarity: 0.5846\n",
      "\n",
      "France : wine :: England : ?\n",
      "Candidate: wine - Similarity: 0.6376\n",
      "Candidate: tea - Similarity: 0.5303\n",
      "Candidate: england - Similarity: 0.5273\n",
      "\n",
      "\n",
      "Dimension: 300\n",
      "\n",
      "dog : puppy :: cat : ?\n",
      "Candidate: puppy - Similarity: 0.7050\n",
      "Candidate: cat - Similarity: 0.6243\n",
      "Candidate: puppies - Similarity: 0.5660\n",
      "\n",
      "speak : speaker :: sing : ?\n",
      "Candidate: speaker - Similarity: 0.6781\n",
      "Candidate: sing - Similarity: 0.5948\n",
      "Candidate: sang - Similarity: 0.4773\n",
      "\n",
      "France : French :: England : ?\n",
      "Candidate: england - Similarity: 0.7325\n",
      "Candidate: english - Similarity: 0.6759\n",
      "Candidate: british - Similarity: 0.5193\n",
      "\n",
      "France : wine :: England : ?\n",
      "Candidate: wine - Similarity: 0.6390\n",
      "Candidate: wines - Similarity: 0.5128\n",
      "Candidate: england - Similarity: 0.4842\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embeddings_directory = 'C:/Users/sures/OneDrive - DePaul University/Desktop/glove.6B'\n",
    "\n",
    "def load_glove_embeddings(file_path, dimension):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:dimension + 1], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def load_glove_embeddings_by_dimension(directory_path, dimensions):\n",
    "    embeddings = {}\n",
    "    for dimension in dimensions:\n",
    "        file_path = f\"{directory_path}/glove.6B.{dimension}d.txt\"\n",
    "        embeddings[dimension] = load_glove_embeddings(file_path, dimension)\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings_directory = 'C:/Users/sures/OneDrive - DePaul University/Desktop/glove.6B'\n",
    "desired_dimensions = [50, 100, 200, 300]\n",
    "\n",
    "# Load the GloVe embeddings for each desired dimension\n",
    "glove_embeddings = load_glove_embeddings_by_dimension(glove_embeddings_directory, desired_dimensions)\n",
    "\n",
    "# Function to calculate cosine similarity between two vectors\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Function to find the most similar word to a given word\n",
    "def find_most_similar_word(word, embeddings, exclude_variants=True):\n",
    "    target_vector = embeddings.get(word.lower())\n",
    "    if target_vector is None:\n",
    "        return f\"No embedding found for '{word}'.\"\n",
    "    \n",
    "    similarities = []\n",
    "    for w, vector in embeddings.items():\n",
    "        if exclude_variants and w.lower().startswith(word.lower()):\n",
    "            continue\n",
    "        if w.lower() == word.lower():\n",
    "            continue\n",
    "        sim = cosine_similarity(target_vector, vector)\n",
    "        similarities.append((w, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    most_similar_word, similarity = similarities[0]\n",
    "    return f\"The most similar word to '{word}' is '{most_similar_word}' with a cosine similarity of {similarity:.4f}.\"\n",
    "\n",
    "# Words for finding most similar\n",
    "words = ['dog', 'whale', 'before', 'however', 'fabricate']\n",
    "\n",
    "# Find most similar words for each input word for each desired dimension\n",
    "for dimension in desired_dimensions:\n",
    "    print(f\"Dimension: {dimension}\")\n",
    "    embeddings = glove_embeddings[dimension]\n",
    "    print()\n",
    "    for word in words:\n",
    "        result = find_most_similar_word(word, embeddings)\n",
    "        print(result)\n",
    "    print()\n",
    "\n",
    "# Function to perform analogy using vector addition and subtraction\n",
    "def analogy(a, b, a_star, embeddings):\n",
    "    a_vec = embeddings.get(a.lower())\n",
    "    b_vec = embeddings.get(b.lower())\n",
    "    a_star_vec = embeddings.get(a_star.lower())\n",
    "    \n",
    "    if a_vec is None or b_vec is None or a_star_vec is None:\n",
    "        return f\"One or more words not found in embeddings.\"\n",
    "    \n",
    "    target_vec = b_vec - a_vec + a_star_vec\n",
    "    similarities = []\n",
    "    for w, vector in embeddings.items():\n",
    "        sim = cosine_similarity(target_vec, vector)\n",
    "        similarities.append((w, sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_candidates = similarities[:3]\n",
    "    return top_candidates\n",
    "\n",
    "# Analogies\n",
    "analogies = [('dog', 'puppy', 'cat'), ('speak', 'speaker', 'sing'), ('France', 'French', 'England'), ('France', 'wine', 'England')]\n",
    "\n",
    "# Perform analogies and find top candidates for each desired dimension\n",
    "for dimension in desired_dimensions:\n",
    "    print(f\"Dimension: {dimension}\")\n",
    "    embeddings = glove_embeddings[dimension]\n",
    "    print()\n",
    "    for analogy_words in analogies:\n",
    "        a, b, a_star = analogy_words\n",
    "        candidates = analogy(a, b, a_star, embeddings)\n",
    "        print(f\"{a} : {b} :: {a_star} : ?\")\n",
    "        for candidate, similarity in candidates:\n",
    "            print(f\"Candidate: {candidate} - Similarity: {similarity:.4f}\")\n",
    "        print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
