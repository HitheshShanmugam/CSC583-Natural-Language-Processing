{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC583\n",
    "# HW6\n",
    "# Hithesh Shanmugam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.0.1+cpu)\n",
      "    Python  3.8.10 (you have 3.8.5)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to be a one-woman league, right? Maybe I will. (laughs)\n",
      "\n",
      "\n",
      "(laughs) And there's also\n",
      "I am going to be a little nervous about going to the bathroom. It Thaksin' says. It's the best thing to remember if you\n",
      "I am going to be a man, but also a woman of the Lord. And then, the Lord will be his wife to me.‚Äù\n"
     ]
    }
   ],
   "source": [
    "# Create the text generation pipeline with the DistilGPT-2 model\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "# Generate text using the pipeline\n",
    "res = generator(\"I am going to be a\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# Print the generated text\n",
    "for text in res:\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to be a great captain with great leadership and a passion for the game, but my goal will be to be an excellent coach with a\n",
      "I am going to be a doctor,\" says her father. \"And you have nothing to do with it.\"\n",
      "\n",
      "\"He was always the one\n",
      "I am going to be a doctor, and I am going to be an entrepreneur,\" Cruz told reporters on Monday, while discussing his economic plans, adding\n"
     ]
    }
   ],
   "source": [
    "# Create the text generation pipeline with the GPT-2 model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text using the pipeline\n",
    "res = generator(\"I am going to be a\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# Print the generated text\n",
    "for text in res:\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to be a president very different from the alternative Donald Trump is offering!\"\n",
      "\n",
      "The Republican nominee won the election over Hillary Clinton last November\n",
      "I am going to be a very busy man in June.\"\n",
      "\n",
      "He added: \"I've got my own work and I'm also going through\n",
      "I am going to be a human chump, I am going to make excuses and try to justify my own crimes. Let me just remind you that\n"
     ]
    }
   ],
   "source": [
    "# Create the text generation pipeline with the GPT-2 large model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2-large\")\n",
    "\n",
    "# Generate text using the pipeline\n",
    "res = generator(\"I am going to be a\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# Print the generated text\n",
    "for text in res:\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am going to be a little over optimistic for a minute as to how serious the allegations could be.\"\n",
      "\n",
      "The latest revelation in the telegraph\n",
      "I am going to be a little bit of a bit of a fool this season. I don't think I'm up to the level that I want\n",
      "I am going to be a little bit more specific about this. You saw yesterday you guys put that video, that video was a lot more detail than\n"
     ]
    }
   ],
   "source": [
    "# Create the text generation pipeline with the GPT-2 medium model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2-medium\")\n",
    "\n",
    "# Generate text using the pipeline\n",
    "res = generator(\"I am going to be a\", max_length=30, num_return_sequences=3)\n",
    "\n",
    "# Print the generated text\n",
    "for text in res:\n",
    "    print(text['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] Tokenizer, Model and Search/Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT2LMHeadModel\n",
      "\n",
      "Search/Sampling Method: greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to be fine. I am not feeling well and I am going to be fine. I am not feeling\n",
      "\n",
      "Search/Sampling Method: beam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to be fine. I am not feeling well and I am going to be fine. I am not feeling\n",
      "\n",
      "Search/Sampling Method: top_k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to be fine. I am not feeling well and I am going to be fine. I am not feeling\n",
      "\n",
      "Search/Sampling Method: top_p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to be fine. I am not feeling well and I am going to be fine. I am not feeling\n",
      "Model: GPT2LMHeadModel\n",
      "\n",
      "Search/Sampling Method: greedy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to go to bed. I am going to go to bed. I am going to go to bed.\n",
      "\n",
      "Search/Sampling Method: beam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to go to bed. I am going to go to bed. I am going to go to bed.\n",
      "\n",
      "Search/Sampling Method: top_k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am not feeling well and I am going to go to bed. I am going to go to bed. I am going to go to bed.\n",
      "\n",
      "Search/Sampling Method: top_p\n",
      "I am not feeling well and I am going to go to bed. I am going to go to bed. I am going to go to bed.\n"
     ]
    }
   ],
   "source": [
    "# Define the search and sampling methods\n",
    "search_methods = [\"greedy\", \"beam\", \"top_k\", \"top_p\"]\n",
    "\n",
    "# Load the tokenizer and models\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "models = [\n",
    "    GPT2LMHeadModel.from_pretrained(\"distilgpt2\"),\n",
    "    AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "]\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"I am not feeling well and I am going to\"\n",
    "\n",
    "# Generate text for each model and search/sampling method\n",
    "for model in models:\n",
    "    print(f\"Model: {model.config.architectures[0]}\")\n",
    "    for method in search_methods:\n",
    "        print(f\"\\nSearch/Sampling Method: {method}\")\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        input_ids = input_ids.to(model.device)\n",
    "\n",
    "        # Set the decoding parameters based on the search/sampling method\n",
    "        if method == \"greedy\":\n",
    "            output = model.generate(input_ids, max_length=30, num_return_sequences=1)\n",
    "        elif method == \"beam\":\n",
    "            output = model.generate(input_ids, max_length=30, num_return_sequences=1, num_beams=5)\n",
    "        elif method == \"top_k\":\n",
    "            output = model.generate(input_ids, max_length=30, num_return_sequences=1, top_k=50)\n",
    "        elif method == \"top_p\":\n",
    "            output = model.generate(input_ids, max_length=30, num_return_sequences=1, top_p=0.9)\n",
    "\n",
    "        # Decode and print the generated text\n",
    "        generated_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        for text in generated_text:\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for model 1: 1.786506175994873\n",
      "Perplexity for model 2: 1.6305392980575562\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Select two general language models\n",
    "model1 = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer1 = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer1.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "\n",
    "model2 = GPT2LMHeadModel.from_pretrained(\"gpt2-large\")\n",
    "tokenizer2 = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\n",
    "tokenizer2.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token\n",
    "\n",
    "# Step 2: Prepare the test text\n",
    "test_text_file = \"C:/Users/sures/OneDrive - DePaul University/Desktop/CSC 583/war-and-peace.txt\"\n",
    "\n",
    "with open(test_text_file, \"r\", encoding=\"latin-1\") as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "# Step 3: Encode the test text\n",
    "encodings = tokenizer1.batch_encode_plus(\n",
    "    [test_text],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = encodings[\"input_ids\"]\n",
    "attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "batch_size = 8  # Adjust batch size based on available resources\n",
    "num_samples = input_ids.shape[0]\n",
    "num_batches = num_samples // batch_size + 1\n",
    "\n",
    "# Step 4: Calculate perplexity for each model\n",
    "perplexity1 = 0.0\n",
    "perplexity2 = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "\n",
    "        input_ids_batch = input_ids[start:end].to(model1.device)\n",
    "        attention_mask_batch = attention_mask[start:end].to(model1.device)\n",
    "\n",
    "        outputs1 = model1(input_ids_batch, attention_mask=attention_mask_batch, labels=input_ids_batch)\n",
    "        loss1 = outputs1.loss\n",
    "        perplexity1 += loss1.item()\n",
    "\n",
    "        outputs2 = model2(input_ids_batch, attention_mask=attention_mask_batch, labels=input_ids_batch)\n",
    "        loss2 = outputs2.loss\n",
    "        perplexity2 += loss2.item()\n",
    "\n",
    "perplexity1 = torch.exp(torch.tensor(perplexity1 / num_samples))\n",
    "perplexity2 = torch.exp(torch.tensor(perplexity2 / num_samples))\n",
    "\n",
    "# Step 5: Analyze and compare perplexity values\n",
    "print(\"Perplexity for model 1:\", perplexity1.item())\n",
    "print(\"Perplexity for model 2:\", perplexity2.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
