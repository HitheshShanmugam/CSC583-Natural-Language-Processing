{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC-583\n",
    "# HW-3\n",
    "# Hithesh Shanmugam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1-Baseline lexicon-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 1600\n",
      "Number of test files: 400\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# set the paths to the directories containing the positive and negative review folders\n",
    "pos_dir = 'C:/Users/sures/OneDrive - DePaul University/Desktop/pos'\n",
    "neg_dir = 'C:/Users/sures/OneDrive - DePaul University/Desktop/neg'\n",
    "\n",
    "# create empty lists to store the file paths for training and test sets\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "# iterate through the positive and negative review folders\n",
    "for label, directory in [('pos', pos_dir), ('neg', neg_dir)]:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('cv8') or filename.startswith('cv9'):\n",
    "            # add file to test set\n",
    "            test_files.append(os.path.join(directory, filename))\n",
    "        else:\n",
    "            # add file to training set\n",
    "            train_files.append(os.path.join(directory, filename))\n",
    "\n",
    "print('Number of training files:', len(train_files))\n",
    "print('Number of test files:', len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sets for the pos and neg lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a set of positive and negative words from the sentiment lexicon\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "with open('C:/Users/sures/OneDrive - DePaul University/Desktop/positive-words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(';'):\n",
    "            positive_words.add(line.strip())\n",
    "with open('C:/Users/sures/OneDrive - DePaul University/Desktop/negative-words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(';'):\n",
    "            negative_words.add(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying and evaluating the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.75\n",
      "Recall: 0.64\n",
      "F1 Score: 0.69\n",
      "Accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "# classify the documents in the training set\n",
    "train_results = []\n",
    "for file in train_files:\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # keeping count of positive and negative word counts\n",
    "        num_pos_words = sum(1 for token in tokens if token in positive_words)\n",
    "        num_neg_words = sum(1 for token in tokens if token in negative_words)\n",
    "        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            train_results.append(1)\n",
    "        else:\n",
    "            train_results.append(0)\n",
    "\n",
    "# classify the documents in the testing set\n",
    "test_results = []\n",
    "for file in test_files:\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # keeping count of positive and negative word counts\n",
    "        num_pos_words = sum(1 for token in tokens if token in positive_words)\n",
    "        num_neg_words = sum(1 for token in tokens if token in negative_words)\n",
    "        \n",
    "        if num_pos_words > num_neg_words:\n",
    "            test_results.append(1)\n",
    "        else:\n",
    "            test_results.append(0)\n",
    "\n",
    "# compute precision, recall, f1 score, and accuracy on the test set\n",
    "tp = sum([1 for i in range(len(test_files)) if test_results[i] == 1 and 'pos' in test_files[i]])\n",
    "fp = sum([1 for i in range(len(test_files)) if test_results[i] == 1 and 'neg' in test_files[i]])\n",
    "tn = sum([1 for i in range(len(test_files)) if test_results[i] == 0 and 'neg' in test_files[i]])\n",
    "fn = sum([1 for i in range(len(test_files)) if test_results[i] == 0 and 'pos' in test_files[i]])\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "print(\"Precision: {:.2f}\".format(precision))\n",
    "print(\"Recall: {:.2f}\".format(recall))\n",
    "print(\"F1 Score: {:.2f}\".format(f1_score))\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 1600\n",
      "Number of test files: 400\n"
     ]
    }
   ],
   "source": [
    "# Set the paths to the directories containing the positive and negative review folders\n",
    "pos_dir = 'C:/Users/sures/OneDrive - DePaul University/Desktop/pos'\n",
    "neg_dir = 'C:/Users/sures/OneDrive - DePaul University/Desktop/neg'\n",
    "\n",
    "# Create empty lists to store the file paths for training and test sets\n",
    "train_files = []\n",
    "test_files = []\n",
    "\n",
    "# Iterate through the positive and negative review folders\n",
    "for label, directory in [('pos', pos_dir), ('neg', neg_dir)]:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.startswith('cv8') or filename.startswith('cv9'):\n",
    "            # Add file to test set\n",
    "            test_files.append(os.path.join(directory, filename))\n",
    "        else:\n",
    "            # Add file to training set\n",
    "            train_files.append(os.path.join(directory, filename))\n",
    "\n",
    "print('Number of training files:', len(train_files))\n",
    "print('Number of test files:', len(test_files))\n",
    "\n",
    "# Function to get the last paragraph\n",
    "def get_last_paragraph(text):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    if len(paragraphs) > 1:\n",
    "        return paragraphs[-2]\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def extract_features(text, last_paragraph=True):\n",
    "    # Get the last paragraph\n",
    "    if last_paragraph:\n",
    "        last_paragraph_text = get_last_paragraph(text)\n",
    "    \n",
    "    # Get the word tokens\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Extract features\n",
    "    features = {}\n",
    "    # Feature 1: Number of tokens in the review\n",
    "    features['num_tokens'] = len(tokens)\n",
    "    # Feature 2: Number of positive words\n",
    "    features['num_positive'] = sum([1 for token in tokens if token in positive_words])\n",
    "    # Feature 3: Number of negative words\n",
    "    features['num_negative'] = sum([1 for token in tokens if token in negative_words])\n",
    "    # Feature 4: Number of exclamation marks\n",
    "    features['num_exclamation'] = text.count('!')\n",
    "    # Feature 5: Number of question marks\n",
    "    features['num_question'] = text.count('?')\n",
    "    # Feature 6: Number of words in last paragraph\n",
    "    if last_paragraph:\n",
    "        features['num_last_paragraph'] = len(word_tokenize(last_paragraph_text))\n",
    "    # Feature 7: Number of words in the longest sentence\n",
    "    sentences = sent_tokenize(text)\n",
    "    features['num_longest_sentence'] = max([len(word_tokenize(sent)) for sent in sentences])\n",
    "    # Feature 8: Number of unique words\n",
    "    unique_words = set(tokens)\n",
    "    features['num_unique_words'] = len(unique_words)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def preprocess(files, last_paragraph=True):\n",
    "    dataset = []\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            label = 1 if 'pos' in file else 0\n",
    "            features = extract_features(text, last_paragraph=last_paragraph)\n",
    "            dataset.append((features, label))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "train_data = preprocess(train_files, last_paragraph=True)\n",
    "test_data = preprocess(test_files, last_paragraph=True)\n",
    "\n",
    "# Logistic regression classifier begins\n",
    "class LogisticRegression:\n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weights = np.zeros((num_features, 32))\n",
    "        self.bias = 0\n",
    "\n",
    "    # Sigmoid function to calculate the sigmoid \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        y_pred = self.sigmoid(z)\n",
    "        y_pred = y_pred.reshape(-1, 1)  # Reshape to (num_samples, 1)\n",
    "        return y_pred\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        epsilon = 1e-12\n",
    "        y_pred = np.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        loss = - y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
    "        return loss\n",
    "\n",
    "    def train(self, x, y, num_epochs=100, batch_size=32, lr=0.01):\n",
    "        num_batches = int(np.ceil(len(x) / batch_size))\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Shuffle the training data for each epoch\n",
    "            indices = np.random.permutation(len(x))\n",
    "            x = x[indices]\n",
    "            y = y[indices]\n",
    "            \n",
    "            # Split the training data into batches\n",
    "            for batch in range(num_batches):\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                x_batch = x[start:end]\n",
    "                y_batch = y[start:end]\n",
    "                \n",
    "                # Convert a list of weights to a NumPy array\n",
    "                self.weights = np.array(self.weights)\n",
    "                # Compute the predictions and gradients for the current batch\n",
    "                z = np.dot(np.array(x_batch), self.weights) + self.bias\n",
    "                y_pred = self.sigmoid(z)\n",
    "                loss_grad = y_pred - y_batch\n",
    "                weights_grad = np.dot(x_batch.T, loss_grad) / batch_size\n",
    "                bias_grad = np.sum(loss_grad) / batch_size\n",
    "                \n",
    "                # Update the weights and bias using mini-batch gradient descent\n",
    "                self.weights -= lr * weights_grad\n",
    "                self.bias -= lr * bias_grad\n",
    "                \n",
    "                # Compute the loss for the current batch\n",
    "                batch_loss = np.mean(self.cross_entropy_loss(y_pred, y_batch))\n",
    "                epoch_loss += batch_loss\n",
    "            \n",
    "            # Compute the average loss for the epoch\n",
    "            epoch_loss /= num_batches\n",
    "            losses.append(epoch_loss)\n",
    "            \n",
    "            # Print the epoch number and average loss for the epoch\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}: loss={epoch_loss:.4f}\")\n",
    "            \n",
    "        return self.weights, self.bias, losses\n",
    "\n",
    "    def evaluate(self, x, y):\n",
    "        pred_y = self.predict(x)\n",
    "        pred_y[pred_y >= 0.5] = 1\n",
    "        pred_y[pred_y < 0.5] = 0\n",
    "        accuracy = np.mean(pred_y == y)\n",
    "        tp = np.sum((pred_y == 1) & (y == 1))\n",
    "        fp = np.sum((pred_y == 1) & (y == 0))\n",
    "        fn = np.sum((pred_y == 0) & (y == 1))\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# Define the features to be used\n",
    "features = ['num_tokens', 'num_positive', 'num_negative', 'num_exclamation', 'num_question', 'num_last_paragraph', 'num_longest_sentence', 'num_unique_words' ]\n",
    "\n",
    "# Preprocess the data\n",
    "train_data = preprocess(train_files, features)\n",
    "test_data = preprocess(test_files, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the labels and data separately for testing and training\n",
    "train_x = np.array([[sample[0]['num_tokens'], sample[0]['num_positive'], sample[0]['num_negative'], sample[0]['num_exclamation'], sample[0]['num_question'], sample[0]['num_last_paragraph'], sample[0]['num_longest_sentence'], sample[0]['num_unique_words']] for sample in train_data])\n",
    "train_y = np.array([sample[1] for sample in train_data])\n",
    "test_x = np.array([[sample[0]['num_tokens'], sample[0]['num_positive'], sample[0]['num_negative'], sample[0]['num_exclamation'], sample[0]['num_question'], sample[0]['num_last_paragraph'], sample[0]['num_longest_sentence'], sample[0]['num_unique_words']] for sample in test_data])\n",
    "test_y = np.array([sample[1] for sample in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-f9dcfccbd981>:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: loss=13.8634\n",
      "Epoch 2/100: loss=12.9380\n",
      "Epoch 3/100: loss=13.8001\n",
      "Epoch 4/100: loss=13.6662\n",
      "Epoch 5/100: loss=13.2703\n",
      "Epoch 6/100: loss=13.9232\n",
      "Epoch 7/100: loss=14.0879\n",
      "Epoch 8/100: loss=14.0971\n",
      "Epoch 9/100: loss=13.7216\n",
      "Epoch 10/100: loss=13.9732\n",
      "Epoch 11/100: loss=13.8950\n",
      "Epoch 12/100: loss=13.9892\n",
      "Epoch 13/100: loss=12.8483\n",
      "Epoch 14/100: loss=14.0365\n",
      "Epoch 15/100: loss=14.0245\n",
      "Epoch 16/100: loss=13.9342\n",
      "Epoch 17/100: loss=13.4508\n",
      "Epoch 18/100: loss=13.6826\n",
      "Epoch 19/100: loss=14.4313\n",
      "Epoch 20/100: loss=13.5179\n",
      "Epoch 21/100: loss=13.7456\n",
      "Epoch 22/100: loss=13.8391\n",
      "Epoch 23/100: loss=13.9687\n",
      "Epoch 24/100: loss=14.0636\n",
      "Epoch 25/100: loss=14.7791\n",
      "Epoch 26/100: loss=13.6299\n",
      "Epoch 27/100: loss=13.7478\n",
      "Epoch 28/100: loss=13.8326\n",
      "Epoch 29/100: loss=13.4038\n",
      "Epoch 30/100: loss=13.8248\n",
      "Epoch 31/100: loss=13.5498\n",
      "Epoch 32/100: loss=13.8447\n",
      "Epoch 33/100: loss=14.5837\n",
      "Epoch 34/100: loss=13.2080\n",
      "Epoch 35/100: loss=14.2987\n",
      "Epoch 36/100: loss=13.3488\n",
      "Epoch 37/100: loss=13.6158\n",
      "Epoch 38/100: loss=13.7902\n",
      "Epoch 39/100: loss=13.3412\n",
      "Epoch 40/100: loss=14.5357\n",
      "Epoch 41/100: loss=13.7804\n",
      "Epoch 42/100: loss=13.5666\n",
      "Epoch 43/100: loss=13.3496\n",
      "Epoch 44/100: loss=13.9558\n",
      "Epoch 45/100: loss=13.5733\n",
      "Epoch 46/100: loss=13.8714\n",
      "Epoch 47/100: loss=13.5297\n",
      "Epoch 48/100: loss=13.7419\n",
      "Epoch 49/100: loss=14.5405\n",
      "Epoch 50/100: loss=13.7073\n",
      "Epoch 51/100: loss=14.4042\n",
      "Epoch 52/100: loss=13.2774\n",
      "Epoch 53/100: loss=14.3416\n",
      "Epoch 54/100: loss=13.4423\n",
      "Epoch 55/100: loss=13.2900\n",
      "Epoch 56/100: loss=13.3385\n",
      "Epoch 57/100: loss=13.7775\n",
      "Epoch 58/100: loss=13.8324\n",
      "Epoch 59/100: loss=13.4833\n",
      "Epoch 60/100: loss=13.9785\n",
      "Epoch 61/100: loss=13.6837\n",
      "Epoch 62/100: loss=14.0403\n",
      "Epoch 63/100: loss=14.5192\n",
      "Epoch 64/100: loss=14.1068\n",
      "Epoch 65/100: loss=13.4552\n",
      "Epoch 66/100: loss=13.6124\n",
      "Epoch 67/100: loss=13.8855\n",
      "Epoch 68/100: loss=14.5066\n",
      "Epoch 69/100: loss=13.8638\n",
      "Epoch 70/100: loss=13.7337\n",
      "Epoch 71/100: loss=13.5331\n",
      "Epoch 72/100: loss=14.2263\n",
      "Epoch 73/100: loss=13.4220\n",
      "Epoch 74/100: loss=13.9005\n",
      "Epoch 75/100: loss=13.6433\n",
      "Epoch 76/100: loss=14.3290\n",
      "Epoch 77/100: loss=14.1385\n",
      "Epoch 78/100: loss=13.5522\n",
      "Epoch 79/100: loss=14.0474\n",
      "Epoch 80/100: loss=13.7892\n",
      "Epoch 81/100: loss=14.1970\n",
      "Epoch 82/100: loss=13.8224\n",
      "Epoch 83/100: loss=13.8375\n",
      "Epoch 84/100: loss=13.7695\n",
      "Epoch 85/100: loss=13.5169\n",
      "Epoch 86/100: loss=13.8225\n",
      "Epoch 87/100: loss=13.1478\n",
      "Epoch 88/100: loss=14.1834\n",
      "Epoch 89/100: loss=14.2301\n",
      "Epoch 90/100: loss=14.4197\n",
      "Epoch 91/100: loss=13.3993\n",
      "Epoch 92/100: loss=13.8315\n",
      "Epoch 93/100: loss=13.6940\n",
      "Epoch 94/100: loss=13.5581\n",
      "Epoch 95/100: loss=13.5081\n",
      "Epoch 96/100: loss=14.4527\n",
      "Epoch 97/100: loss=14.4223\n",
      "Epoch 98/100: loss=13.2689\n",
      "Epoch 99/100: loss=14.2030\n",
      "Epoch 100/100: loss=13.9302\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.5625\n",
      "F1 score: 0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "log_reg = LogisticRegression(num_features=len(features))\n",
    "weights, bias, loss = log_reg.train(train_x, train_y, num_epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1_score = log_reg.evaluate(test_x, test_y)\n",
    "print('Accuracy:', accuracy)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 score:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: loss=13.5686\n",
      "Epoch 2/100: loss=14.3139\n",
      "Epoch 3/100: loss=14.2889\n",
      "Epoch 4/100: loss=13.7936\n",
      "Epoch 5/100: loss=13.7368\n",
      "Epoch 6/100: loss=13.7474\n",
      "Epoch 7/100: loss=13.6964\n",
      "Epoch 8/100: loss=13.3659\n",
      "Epoch 9/100: loss=13.6179\n",
      "Epoch 10/100: loss=13.5810\n",
      "Epoch 11/100: loss=14.0588\n",
      "Epoch 12/100: loss=14.3769\n",
      "Epoch 13/100: loss=13.7891\n",
      "Epoch 14/100: loss=13.6848\n",
      "Epoch 15/100: loss=13.9144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-f9dcfccbd981>:85: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: loss=14.2063\n",
      "Epoch 17/100: loss=13.1620\n",
      "Epoch 18/100: loss=14.3511\n",
      "Epoch 19/100: loss=13.1247\n",
      "Epoch 20/100: loss=14.4036\n",
      "Epoch 21/100: loss=13.7724\n",
      "Epoch 22/100: loss=14.0434\n",
      "Epoch 23/100: loss=13.8547\n",
      "Epoch 24/100: loss=14.0570\n",
      "Epoch 25/100: loss=14.2449\n",
      "Epoch 26/100: loss=14.1244\n",
      "Epoch 27/100: loss=13.7310\n",
      "Epoch 28/100: loss=13.4969\n",
      "Epoch 29/100: loss=14.3189\n",
      "Epoch 30/100: loss=13.4690\n",
      "Epoch 31/100: loss=13.8341\n",
      "Epoch 32/100: loss=13.8035\n",
      "Epoch 33/100: loss=13.3573\n",
      "Epoch 34/100: loss=14.0350\n",
      "Epoch 35/100: loss=13.7090\n",
      "Epoch 36/100: loss=13.7516\n",
      "Epoch 37/100: loss=13.9365\n",
      "Epoch 38/100: loss=12.8722\n",
      "Epoch 39/100: loss=14.2356\n",
      "Epoch 40/100: loss=14.1286\n",
      "Epoch 41/100: loss=14.3425\n",
      "Epoch 42/100: loss=14.1203\n",
      "Epoch 43/100: loss=13.9220\n",
      "Epoch 44/100: loss=14.0362\n",
      "Epoch 45/100: loss=13.2676\n",
      "Epoch 46/100: loss=14.3185\n",
      "Epoch 47/100: loss=13.3978\n",
      "Epoch 48/100: loss=14.3149\n",
      "Epoch 49/100: loss=13.8854\n",
      "Epoch 50/100: loss=13.3420\n",
      "Epoch 51/100: loss=13.6076\n",
      "Epoch 52/100: loss=13.9426\n",
      "Epoch 53/100: loss=13.3528\n",
      "Epoch 54/100: loss=14.0399\n",
      "Epoch 55/100: loss=13.8591\n",
      "Epoch 56/100: loss=14.1255\n",
      "Epoch 57/100: loss=13.4837\n",
      "Epoch 58/100: loss=13.5137\n",
      "Epoch 59/100: loss=13.4982\n",
      "Epoch 60/100: loss=13.4678\n",
      "Epoch 61/100: loss=14.0126\n",
      "Epoch 62/100: loss=13.5752\n",
      "Epoch 63/100: loss=13.5984\n",
      "Epoch 64/100: loss=14.2261\n",
      "Epoch 65/100: loss=13.8455\n",
      "Epoch 66/100: loss=14.0632\n",
      "Epoch 67/100: loss=13.7572\n",
      "Epoch 68/100: loss=14.0176\n",
      "Epoch 69/100: loss=13.9124\n",
      "Epoch 70/100: loss=14.5877\n",
      "Epoch 71/100: loss=13.7456\n",
      "Epoch 72/100: loss=13.9244\n",
      "Epoch 73/100: loss=13.7880\n",
      "Epoch 74/100: loss=13.4860\n",
      "Epoch 75/100: loss=13.5535\n",
      "Epoch 76/100: loss=12.9995\n",
      "Epoch 77/100: loss=13.8752\n",
      "Epoch 78/100: loss=13.8830\n",
      "Epoch 79/100: loss=13.5463\n",
      "Epoch 80/100: loss=13.1969\n",
      "Epoch 81/100: loss=14.3404\n",
      "Epoch 82/100: loss=13.8022\n",
      "Epoch 83/100: loss=14.1645\n",
      "Epoch 84/100: loss=13.7501\n",
      "Epoch 85/100: loss=14.4916\n",
      "Epoch 86/100: loss=14.0750\n",
      "Epoch 87/100: loss=13.0323\n",
      "Epoch 88/100: loss=13.8599\n",
      "Epoch 89/100: loss=13.8590\n",
      "Epoch 90/100: loss=13.4911\n",
      "Epoch 91/100: loss=14.1298\n",
      "Epoch 92/100: loss=13.5841\n",
      "Epoch 93/100: loss=13.8587\n",
      "Epoch 94/100: loss=13.4225\n",
      "Epoch 95/100: loss=13.1439\n",
      "Epoch 96/100: loss=13.7058\n",
      "Epoch 97/100: loss=13.5712\n",
      "Epoch 98/100: loss=13.3445\n",
      "Epoch 99/100: loss=13.5966\n",
      "Epoch 100/100: loss=13.6046\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_tokens:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.34375\n",
      "F1 Score: 0.4074074074074074\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=14.6150\n",
      "Epoch 2/100: loss=13.7845\n",
      "Epoch 3/100: loss=13.3780\n",
      "Epoch 4/100: loss=13.8554\n",
      "Epoch 5/100: loss=13.6172\n",
      "Epoch 6/100: loss=13.8076\n",
      "Epoch 7/100: loss=13.3180\n",
      "Epoch 8/100: loss=13.5408\n",
      "Epoch 9/100: loss=13.9036\n",
      "Epoch 10/100: loss=14.1135\n",
      "Epoch 11/100: loss=13.9528\n",
      "Epoch 12/100: loss=13.6785\n",
      "Epoch 13/100: loss=14.0161\n",
      "Epoch 14/100: loss=14.0252\n",
      "Epoch 15/100: loss=13.8163\n",
      "Epoch 16/100: loss=14.3453\n",
      "Epoch 17/100: loss=13.3959\n",
      "Epoch 18/100: loss=13.3296\n",
      "Epoch 19/100: loss=13.7883\n",
      "Epoch 20/100: loss=13.6355\n",
      "Epoch 21/100: loss=13.6943\n",
      "Epoch 22/100: loss=13.7800\n",
      "Epoch 23/100: loss=14.1704\n",
      "Epoch 24/100: loss=14.1914\n",
      "Epoch 25/100: loss=14.3585\n",
      "Epoch 26/100: loss=13.6630\n",
      "Epoch 27/100: loss=13.7508\n",
      "Epoch 28/100: loss=14.2193\n",
      "Epoch 29/100: loss=13.8981\n",
      "Epoch 30/100: loss=13.5744\n",
      "Epoch 31/100: loss=13.4068\n",
      "Epoch 32/100: loss=13.6342\n",
      "Epoch 33/100: loss=13.3065\n",
      "Epoch 34/100: loss=14.5888\n",
      "Epoch 35/100: loss=14.1624\n",
      "Epoch 36/100: loss=13.8255\n",
      "Epoch 37/100: loss=13.9081\n",
      "Epoch 38/100: loss=14.2864\n",
      "Epoch 39/100: loss=14.2538\n",
      "Epoch 40/100: loss=14.0268\n",
      "Epoch 41/100: loss=13.6040\n",
      "Epoch 42/100: loss=13.7881\n",
      "Epoch 43/100: loss=14.3340\n",
      "Epoch 44/100: loss=13.4671\n",
      "Epoch 45/100: loss=14.4761\n",
      "Epoch 46/100: loss=13.7867\n",
      "Epoch 47/100: loss=13.7172\n",
      "Epoch 48/100: loss=13.7526\n",
      "Epoch 49/100: loss=13.7635\n",
      "Epoch 50/100: loss=14.1167\n",
      "Epoch 51/100: loss=13.9531\n",
      "Epoch 52/100: loss=13.4001\n",
      "Epoch 53/100: loss=13.9733\n",
      "Epoch 54/100: loss=13.6358\n",
      "Epoch 55/100: loss=14.2972\n",
      "Epoch 56/100: loss=13.6914\n",
      "Epoch 57/100: loss=14.3114\n",
      "Epoch 58/100: loss=13.9594\n",
      "Epoch 59/100: loss=14.4298\n",
      "Epoch 60/100: loss=13.8031\n",
      "Epoch 61/100: loss=13.5033\n",
      "Epoch 62/100: loss=13.7549\n",
      "Epoch 63/100: loss=14.0387\n",
      "Epoch 64/100: loss=13.5496\n",
      "Epoch 65/100: loss=13.9144\n",
      "Epoch 66/100: loss=13.8672\n",
      "Epoch 67/100: loss=13.7929\n",
      "Epoch 68/100: loss=14.1625\n",
      "Epoch 69/100: loss=13.7775\n",
      "Epoch 70/100: loss=14.3332\n",
      "Epoch 71/100: loss=13.5686\n",
      "Epoch 72/100: loss=13.6135\n",
      "Epoch 73/100: loss=13.9976\n",
      "Epoch 74/100: loss=13.4106\n",
      "Epoch 75/100: loss=14.0179\n",
      "Epoch 76/100: loss=13.3374\n",
      "Epoch 77/100: loss=12.9498\n",
      "Epoch 78/100: loss=14.0195\n",
      "Epoch 79/100: loss=13.6041\n",
      "Epoch 80/100: loss=13.4618\n",
      "Epoch 81/100: loss=13.5125\n",
      "Epoch 82/100: loss=13.7418\n",
      "Epoch 83/100: loss=13.7219\n",
      "Epoch 84/100: loss=13.9397\n",
      "Epoch 85/100: loss=13.9085\n",
      "Epoch 86/100: loss=14.0306\n",
      "Epoch 87/100: loss=13.3873\n",
      "Epoch 88/100: loss=14.2055\n",
      "Epoch 89/100: loss=14.4962\n",
      "Epoch 90/100: loss=14.0845\n",
      "Epoch 91/100: loss=13.6484\n",
      "Epoch 92/100: loss=13.4327\n",
      "Epoch 93/100: loss=13.7962\n",
      "Epoch 94/100: loss=13.0608\n",
      "Epoch 95/100: loss=14.4674\n",
      "Epoch 96/100: loss=13.8845\n",
      "Epoch 97/100: loss=13.3442\n",
      "Epoch 98/100: loss=13.5041\n",
      "Epoch 99/100: loss=14.1354\n",
      "Epoch 100/100: loss=13.9023\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_positive:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.53125\n",
      "F1 Score: 0.5151515151515151\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=13.5039\n",
      "Epoch 2/100: loss=14.0276\n",
      "Epoch 3/100: loss=13.5539\n",
      "Epoch 4/100: loss=13.9328\n",
      "Epoch 5/100: loss=14.2046\n",
      "Epoch 6/100: loss=13.1310\n",
      "Epoch 7/100: loss=14.3268\n",
      "Epoch 8/100: loss=14.1906\n",
      "Epoch 9/100: loss=14.1800\n",
      "Epoch 10/100: loss=13.7083\n",
      "Epoch 11/100: loss=14.3254\n",
      "Epoch 12/100: loss=13.7039\n",
      "Epoch 13/100: loss=13.5538\n",
      "Epoch 14/100: loss=13.7556\n",
      "Epoch 15/100: loss=14.2167\n",
      "Epoch 16/100: loss=13.0423\n",
      "Epoch 17/100: loss=14.1904\n",
      "Epoch 18/100: loss=13.3807\n",
      "Epoch 19/100: loss=14.1314\n",
      "Epoch 20/100: loss=13.9588\n",
      "Epoch 21/100: loss=13.9888\n",
      "Epoch 22/100: loss=13.2674\n",
      "Epoch 23/100: loss=13.7704\n",
      "Epoch 24/100: loss=14.1185\n",
      "Epoch 25/100: loss=13.8170\n",
      "Epoch 26/100: loss=14.3257\n",
      "Epoch 27/100: loss=13.7471\n",
      "Epoch 28/100: loss=13.6618\n",
      "Epoch 29/100: loss=14.1255\n",
      "Epoch 30/100: loss=14.5385\n",
      "Epoch 31/100: loss=14.3166\n",
      "Epoch 32/100: loss=14.8121\n",
      "Epoch 33/100: loss=13.8978\n",
      "Epoch 34/100: loss=13.7508\n",
      "Epoch 35/100: loss=13.9227\n",
      "Epoch 36/100: loss=13.5776\n",
      "Epoch 37/100: loss=13.9589\n",
      "Epoch 38/100: loss=14.0889\n",
      "Epoch 39/100: loss=13.6810\n",
      "Epoch 40/100: loss=13.8204\n",
      "Epoch 41/100: loss=14.0130\n",
      "Epoch 42/100: loss=13.6858\n",
      "Epoch 43/100: loss=13.3077\n",
      "Epoch 44/100: loss=13.4298\n",
      "Epoch 45/100: loss=14.2098\n",
      "Epoch 46/100: loss=13.8692\n",
      "Epoch 47/100: loss=13.5923\n",
      "Epoch 48/100: loss=14.4963\n",
      "Epoch 49/100: loss=13.8000\n",
      "Epoch 50/100: loss=13.7701\n",
      "Epoch 51/100: loss=13.8042\n",
      "Epoch 52/100: loss=14.1139\n",
      "Epoch 53/100: loss=13.9555\n",
      "Epoch 54/100: loss=13.7940\n",
      "Epoch 55/100: loss=13.7735\n",
      "Epoch 56/100: loss=14.3001\n",
      "Epoch 57/100: loss=13.8158\n",
      "Epoch 58/100: loss=13.7878\n",
      "Epoch 59/100: loss=13.5720\n",
      "Epoch 60/100: loss=14.0026\n",
      "Epoch 61/100: loss=13.4990\n",
      "Epoch 62/100: loss=13.3543\n",
      "Epoch 63/100: loss=13.4148\n",
      "Epoch 64/100: loss=14.0674\n",
      "Epoch 65/100: loss=13.6864\n",
      "Epoch 66/100: loss=14.2020\n",
      "Epoch 67/100: loss=14.1238\n",
      "Epoch 68/100: loss=13.5936\n",
      "Epoch 69/100: loss=14.3674\n",
      "Epoch 70/100: loss=14.1142\n",
      "Epoch 71/100: loss=12.8753\n",
      "Epoch 72/100: loss=13.8006\n",
      "Epoch 73/100: loss=13.4110\n",
      "Epoch 74/100: loss=13.7231\n",
      "Epoch 75/100: loss=14.1183\n",
      "Epoch 76/100: loss=13.6211\n",
      "Epoch 77/100: loss=14.1549\n",
      "Epoch 78/100: loss=13.8805\n",
      "Epoch 79/100: loss=13.0676\n",
      "Epoch 80/100: loss=13.9433\n",
      "Epoch 81/100: loss=13.6875\n",
      "Epoch 82/100: loss=14.1985\n",
      "Epoch 83/100: loss=13.3622\n",
      "Epoch 84/100: loss=14.1336\n",
      "Epoch 85/100: loss=13.6827\n",
      "Epoch 86/100: loss=13.7887\n",
      "Epoch 87/100: loss=14.1173\n",
      "Epoch 88/100: loss=13.0716\n",
      "Epoch 89/100: loss=13.9519\n",
      "Epoch 90/100: loss=14.5606\n",
      "Epoch 91/100: loss=13.5595\n",
      "Epoch 92/100: loss=14.2431\n",
      "Epoch 93/100: loss=13.4058\n",
      "Epoch 94/100: loss=13.9358\n",
      "Epoch 95/100: loss=14.0288\n",
      "Epoch 96/100: loss=13.8340\n",
      "Epoch 97/100: loss=13.2353\n",
      "Epoch 98/100: loss=13.3161\n",
      "Epoch 99/100: loss=14.1222\n",
      "Epoch 100/100: loss=13.8879\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_negative:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.4375\n",
      "F1 Score: 0.4666666666666667\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=13.2766\n",
      "Epoch 2/100: loss=15.0059\n",
      "Epoch 3/100: loss=13.5428\n",
      "Epoch 4/100: loss=14.4800\n",
      "Epoch 5/100: loss=13.7354\n",
      "Epoch 6/100: loss=13.5081\n",
      "Epoch 7/100: loss=13.9759\n",
      "Epoch 8/100: loss=13.5663\n",
      "Epoch 9/100: loss=13.4795\n",
      "Epoch 10/100: loss=13.6894\n",
      "Epoch 11/100: loss=14.0317\n",
      "Epoch 12/100: loss=13.8048\n",
      "Epoch 13/100: loss=14.5930\n",
      "Epoch 14/100: loss=14.2583\n",
      "Epoch 15/100: loss=13.8465\n",
      "Epoch 16/100: loss=13.3433\n",
      "Epoch 17/100: loss=14.0904\n",
      "Epoch 18/100: loss=14.2307\n",
      "Epoch 19/100: loss=14.5161\n",
      "Epoch 20/100: loss=13.2424\n",
      "Epoch 21/100: loss=13.0903\n",
      "Epoch 22/100: loss=13.5581\n",
      "Epoch 23/100: loss=13.4279\n",
      "Epoch 24/100: loss=14.2342\n",
      "Epoch 25/100: loss=13.0988\n",
      "Epoch 26/100: loss=14.4294\n",
      "Epoch 27/100: loss=13.8958\n",
      "Epoch 28/100: loss=13.5881\n",
      "Epoch 29/100: loss=13.2084\n",
      "Epoch 30/100: loss=13.8142\n",
      "Epoch 31/100: loss=13.8188\n",
      "Epoch 32/100: loss=14.1046\n",
      "Epoch 33/100: loss=13.6593\n",
      "Epoch 34/100: loss=13.7663\n",
      "Epoch 35/100: loss=14.0769\n",
      "Epoch 36/100: loss=13.5675\n",
      "Epoch 37/100: loss=14.3150\n",
      "Epoch 38/100: loss=13.5712\n",
      "Epoch 39/100: loss=13.6051\n",
      "Epoch 40/100: loss=13.8922\n",
      "Epoch 41/100: loss=14.2837\n",
      "Epoch 42/100: loss=13.4609\n",
      "Epoch 43/100: loss=13.8307\n",
      "Epoch 44/100: loss=13.0953\n",
      "Epoch 45/100: loss=13.6352\n",
      "Epoch 46/100: loss=14.6618\n",
      "Epoch 47/100: loss=13.3188\n",
      "Epoch 48/100: loss=14.3797\n",
      "Epoch 49/100: loss=13.6105\n",
      "Epoch 50/100: loss=13.8985\n",
      "Epoch 51/100: loss=13.7689\n",
      "Epoch 52/100: loss=13.7982\n",
      "Epoch 53/100: loss=13.9494\n",
      "Epoch 54/100: loss=14.1541\n",
      "Epoch 55/100: loss=13.3040\n",
      "Epoch 56/100: loss=13.8704\n",
      "Epoch 57/100: loss=13.8764\n",
      "Epoch 58/100: loss=13.3748\n",
      "Epoch 59/100: loss=14.4531\n",
      "Epoch 60/100: loss=13.2107\n",
      "Epoch 61/100: loss=13.7217\n",
      "Epoch 62/100: loss=13.4673\n",
      "Epoch 63/100: loss=14.4561\n",
      "Epoch 64/100: loss=14.2678\n",
      "Epoch 65/100: loss=13.3042\n",
      "Epoch 66/100: loss=13.2272\n",
      "Epoch 67/100: loss=14.2850\n",
      "Epoch 68/100: loss=14.1582\n",
      "Epoch 69/100: loss=13.6933\n",
      "Epoch 70/100: loss=13.2919\n",
      "Epoch 71/100: loss=14.1506\n",
      "Epoch 72/100: loss=13.4012\n",
      "Epoch 73/100: loss=14.2298\n",
      "Epoch 74/100: loss=14.1370\n",
      "Epoch 75/100: loss=13.4559\n",
      "Epoch 76/100: loss=13.5194\n",
      "Epoch 77/100: loss=13.7534\n",
      "Epoch 78/100: loss=14.0278\n",
      "Epoch 79/100: loss=13.4305\n",
      "Epoch 80/100: loss=13.9960\n",
      "Epoch 81/100: loss=13.9518\n",
      "Epoch 82/100: loss=13.3534\n",
      "Epoch 83/100: loss=13.8917\n",
      "Epoch 84/100: loss=13.3622\n",
      "Epoch 85/100: loss=14.2594\n",
      "Epoch 86/100: loss=13.9235\n",
      "Epoch 87/100: loss=13.6391\n",
      "Epoch 88/100: loss=13.3101\n",
      "Epoch 89/100: loss=13.8673\n",
      "Epoch 90/100: loss=13.6820\n",
      "Epoch 91/100: loss=13.3904\n",
      "Epoch 92/100: loss=13.9819\n",
      "Epoch 93/100: loss=13.2881\n",
      "Epoch 94/100: loss=13.2738\n",
      "Epoch 95/100: loss=13.9362\n",
      "Epoch 96/100: loss=14.0226\n",
      "Epoch 97/100: loss=13.6199\n",
      "Epoch 98/100: loss=13.5278\n",
      "Epoch 99/100: loss=13.7660\n",
      "Epoch 100/100: loss=14.1290\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_exclamation:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.625\n",
      "F1 Score: 0.5555555555555556\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=13.1076\n",
      "Epoch 2/100: loss=13.3230\n",
      "Epoch 3/100: loss=13.7038\n",
      "Epoch 4/100: loss=14.0077\n",
      "Epoch 5/100: loss=14.0054\n",
      "Epoch 6/100: loss=12.7963\n",
      "Epoch 7/100: loss=13.8083\n",
      "Epoch 8/100: loss=14.3687\n",
      "Epoch 9/100: loss=13.9141\n",
      "Epoch 10/100: loss=13.9932\n",
      "Epoch 11/100: loss=14.3359\n",
      "Epoch 12/100: loss=14.0028\n",
      "Epoch 13/100: loss=13.6939\n",
      "Epoch 14/100: loss=13.6469\n",
      "Epoch 15/100: loss=13.2291\n",
      "Epoch 16/100: loss=14.1992\n",
      "Epoch 17/100: loss=13.9960\n",
      "Epoch 18/100: loss=13.7955\n",
      "Epoch 19/100: loss=13.5168\n",
      "Epoch 20/100: loss=13.4452\n",
      "Epoch 21/100: loss=13.5636\n",
      "Epoch 22/100: loss=13.7833\n",
      "Epoch 23/100: loss=13.8603\n",
      "Epoch 24/100: loss=13.9121\n",
      "Epoch 25/100: loss=14.1570\n",
      "Epoch 26/100: loss=13.9509\n",
      "Epoch 27/100: loss=14.1222\n",
      "Epoch 28/100: loss=13.3681\n",
      "Epoch 29/100: loss=13.0904\n",
      "Epoch 30/100: loss=14.0672\n",
      "Epoch 31/100: loss=13.8857\n",
      "Epoch 32/100: loss=13.8127\n",
      "Epoch 33/100: loss=14.0086\n",
      "Epoch 34/100: loss=13.9796\n",
      "Epoch 35/100: loss=14.4202\n",
      "Epoch 36/100: loss=13.1656\n",
      "Epoch 37/100: loss=14.2071\n",
      "Epoch 38/100: loss=13.9660\n",
      "Epoch 39/100: loss=13.1001\n",
      "Epoch 40/100: loss=14.2329\n",
      "Epoch 41/100: loss=13.7284\n",
      "Epoch 42/100: loss=14.0434\n",
      "Epoch 43/100: loss=14.1984\n",
      "Epoch 44/100: loss=14.2472\n",
      "Epoch 45/100: loss=14.1226\n",
      "Epoch 46/100: loss=13.6437\n",
      "Epoch 47/100: loss=13.8707\n",
      "Epoch 48/100: loss=13.8373\n",
      "Epoch 49/100: loss=14.0916\n",
      "Epoch 50/100: loss=13.9035\n",
      "Epoch 51/100: loss=14.2805\n",
      "Epoch 52/100: loss=14.3019\n",
      "Epoch 53/100: loss=14.0816\n",
      "Epoch 54/100: loss=14.3299\n",
      "Epoch 55/100: loss=13.4010\n",
      "Epoch 56/100: loss=13.0238\n",
      "Epoch 57/100: loss=13.5841\n",
      "Epoch 58/100: loss=13.5102\n",
      "Epoch 59/100: loss=13.4775\n",
      "Epoch 60/100: loss=13.5329\n",
      "Epoch 61/100: loss=14.1911\n",
      "Epoch 62/100: loss=14.0413\n",
      "Epoch 63/100: loss=13.5265\n",
      "Epoch 64/100: loss=13.6301\n",
      "Epoch 65/100: loss=13.6490\n",
      "Epoch 66/100: loss=14.0597\n",
      "Epoch 67/100: loss=13.7707\n",
      "Epoch 68/100: loss=14.3095\n",
      "Epoch 69/100: loss=13.4515\n",
      "Epoch 70/100: loss=13.7225\n",
      "Epoch 71/100: loss=13.6615\n",
      "Epoch 72/100: loss=13.1967\n",
      "Epoch 73/100: loss=13.3682\n",
      "Epoch 74/100: loss=13.2689\n",
      "Epoch 75/100: loss=13.6472\n",
      "Epoch 76/100: loss=14.1680\n",
      "Epoch 77/100: loss=13.3827\n",
      "Epoch 78/100: loss=14.0493\n",
      "Epoch 79/100: loss=13.5323\n",
      "Epoch 80/100: loss=14.3325\n",
      "Epoch 81/100: loss=14.1857\n",
      "Epoch 82/100: loss=13.5790\n",
      "Epoch 83/100: loss=13.9522\n",
      "Epoch 84/100: loss=14.1012\n",
      "Epoch 85/100: loss=14.1564\n",
      "Epoch 86/100: loss=13.9973\n",
      "Epoch 87/100: loss=14.2990\n",
      "Epoch 88/100: loss=13.9162\n",
      "Epoch 89/100: loss=14.1342\n",
      "Epoch 90/100: loss=13.7509\n",
      "Epoch 91/100: loss=13.8224\n",
      "Epoch 92/100: loss=13.6325\n",
      "Epoch 93/100: loss=13.7067\n",
      "Epoch 94/100: loss=13.5982\n",
      "Epoch 95/100: loss=13.6374\n",
      "Epoch 96/100: loss=13.8954\n",
      "Epoch 97/100: loss=14.0107\n",
      "Epoch 98/100: loss=13.5093\n",
      "Epoch 99/100: loss=13.9772\n",
      "Epoch 100/100: loss=13.8488\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_question:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.53125\n",
      "F1 Score: 0.5151515151515151\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=13.1537\n",
      "Epoch 2/100: loss=13.4205\n",
      "Epoch 3/100: loss=13.4013\n",
      "Epoch 4/100: loss=13.7641\n",
      "Epoch 5/100: loss=13.2254\n",
      "Epoch 6/100: loss=13.2008\n",
      "Epoch 7/100: loss=14.2836\n",
      "Epoch 8/100: loss=13.8663\n",
      "Epoch 9/100: loss=13.5511\n",
      "Epoch 10/100: loss=13.5388\n",
      "Epoch 11/100: loss=13.8941\n",
      "Epoch 12/100: loss=13.8499\n",
      "Epoch 13/100: loss=13.9063\n",
      "Epoch 14/100: loss=13.6427\n",
      "Epoch 15/100: loss=14.0887\n",
      "Epoch 16/100: loss=14.1488\n",
      "Epoch 17/100: loss=13.8543\n",
      "Epoch 18/100: loss=13.5637\n",
      "Epoch 19/100: loss=13.7401\n",
      "Epoch 20/100: loss=13.7209\n",
      "Epoch 21/100: loss=13.8661\n",
      "Epoch 22/100: loss=13.6570\n",
      "Epoch 23/100: loss=14.3214\n",
      "Epoch 24/100: loss=13.7638\n",
      "Epoch 25/100: loss=13.8646\n",
      "Epoch 26/100: loss=13.6802\n",
      "Epoch 27/100: loss=13.6081\n",
      "Epoch 28/100: loss=13.9865\n",
      "Epoch 29/100: loss=13.7738\n",
      "Epoch 30/100: loss=14.0871\n",
      "Epoch 31/100: loss=13.3199\n",
      "Epoch 32/100: loss=13.3364\n",
      "Epoch 33/100: loss=13.7340\n",
      "Epoch 34/100: loss=13.9355\n",
      "Epoch 35/100: loss=14.0421\n",
      "Epoch 36/100: loss=13.8110\n",
      "Epoch 37/100: loss=13.0295\n",
      "Epoch 38/100: loss=12.8060\n",
      "Epoch 39/100: loss=14.2451\n",
      "Epoch 40/100: loss=14.0336\n",
      "Epoch 41/100: loss=14.4316\n",
      "Epoch 42/100: loss=13.7883\n",
      "Epoch 43/100: loss=14.7012\n",
      "Epoch 44/100: loss=14.2211\n",
      "Epoch 45/100: loss=13.4042\n",
      "Epoch 46/100: loss=13.8078\n",
      "Epoch 47/100: loss=13.3631\n",
      "Epoch 48/100: loss=13.3278\n",
      "Epoch 49/100: loss=13.9933\n",
      "Epoch 50/100: loss=13.5447\n",
      "Epoch 51/100: loss=13.7043\n",
      "Epoch 52/100: loss=13.7943\n",
      "Epoch 53/100: loss=14.1211\n",
      "Epoch 54/100: loss=13.5001\n",
      "Epoch 55/100: loss=13.5487\n",
      "Epoch 56/100: loss=13.4038\n",
      "Epoch 57/100: loss=14.2805\n",
      "Epoch 58/100: loss=13.8308\n",
      "Epoch 59/100: loss=14.0862\n",
      "Epoch 60/100: loss=14.2850\n",
      "Epoch 61/100: loss=14.0671\n",
      "Epoch 62/100: loss=13.8328\n",
      "Epoch 63/100: loss=13.7552\n",
      "Epoch 64/100: loss=14.1366\n",
      "Epoch 65/100: loss=14.0796\n",
      "Epoch 66/100: loss=13.8672\n",
      "Epoch 67/100: loss=13.8797\n",
      "Epoch 68/100: loss=13.9674\n",
      "Epoch 69/100: loss=13.4742\n",
      "Epoch 70/100: loss=14.3114\n",
      "Epoch 71/100: loss=13.7407\n",
      "Epoch 72/100: loss=14.0332\n",
      "Epoch 73/100: loss=13.4409\n",
      "Epoch 74/100: loss=13.8832\n",
      "Epoch 75/100: loss=13.9972\n",
      "Epoch 76/100: loss=13.8655\n",
      "Epoch 77/100: loss=13.8716\n",
      "Epoch 78/100: loss=13.8154\n",
      "Epoch 79/100: loss=14.4857\n",
      "Epoch 80/100: loss=14.6508\n",
      "Epoch 81/100: loss=13.7273\n",
      "Epoch 82/100: loss=13.6051\n",
      "Epoch 83/100: loss=13.6538\n",
      "Epoch 84/100: loss=13.4104\n",
      "Epoch 85/100: loss=13.7572\n",
      "Epoch 86/100: loss=14.0895\n",
      "Epoch 87/100: loss=13.9075\n",
      "Epoch 88/100: loss=13.7001\n",
      "Epoch 89/100: loss=13.3192\n",
      "Epoch 90/100: loss=13.6400\n",
      "Epoch 91/100: loss=14.3771\n",
      "Epoch 92/100: loss=13.5783\n",
      "Epoch 93/100: loss=13.9403\n",
      "Epoch 94/100: loss=13.6628\n",
      "Epoch 95/100: loss=13.5565\n",
      "Epoch 96/100: loss=14.3697\n",
      "Epoch 97/100: loss=13.7622\n",
      "Epoch 98/100: loss=13.6261\n",
      "Epoch 99/100: loss=14.0390\n",
      "Epoch 100/100: loss=14.6173\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_last_paragraph:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.374921875\n",
      "F1 Score: 0.42852040360746496\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=13.3943\n",
      "Epoch 2/100: loss=13.4374\n",
      "Epoch 3/100: loss=13.8665\n",
      "Epoch 4/100: loss=13.9084\n",
      "Epoch 5/100: loss=13.3646\n",
      "Epoch 6/100: loss=14.1936\n",
      "Epoch 7/100: loss=13.5417\n",
      "Epoch 8/100: loss=13.7586\n",
      "Epoch 9/100: loss=13.8797\n",
      "Epoch 10/100: loss=13.4902\n",
      "Epoch 11/100: loss=13.8083\n",
      "Epoch 12/100: loss=13.9693\n",
      "Epoch 13/100: loss=14.1133\n",
      "Epoch 14/100: loss=13.1802\n",
      "Epoch 15/100: loss=13.6018\n",
      "Epoch 16/100: loss=13.2272\n",
      "Epoch 17/100: loss=13.8153\n",
      "Epoch 18/100: loss=13.2198\n",
      "Epoch 19/100: loss=12.9389\n",
      "Epoch 20/100: loss=13.7901\n",
      "Epoch 21/100: loss=13.6990\n",
      "Epoch 22/100: loss=14.1265\n",
      "Epoch 23/100: loss=14.0793\n",
      "Epoch 24/100: loss=14.0255\n",
      "Epoch 25/100: loss=14.3544\n",
      "Epoch 26/100: loss=13.4949\n",
      "Epoch 27/100: loss=13.7863\n",
      "Epoch 28/100: loss=13.7142\n",
      "Epoch 29/100: loss=13.6776\n",
      "Epoch 30/100: loss=14.4365\n",
      "Epoch 31/100: loss=13.7184\n",
      "Epoch 32/100: loss=14.2622\n",
      "Epoch 33/100: loss=13.7252\n",
      "Epoch 34/100: loss=13.4477\n",
      "Epoch 35/100: loss=13.5456\n",
      "Epoch 36/100: loss=13.7480\n",
      "Epoch 37/100: loss=14.0610\n",
      "Epoch 38/100: loss=13.7914\n",
      "Epoch 39/100: loss=14.0635\n",
      "Epoch 40/100: loss=13.8850\n",
      "Epoch 41/100: loss=13.5695\n",
      "Epoch 42/100: loss=13.8460\n",
      "Epoch 43/100: loss=13.3117\n",
      "Epoch 44/100: loss=13.9052\n",
      "Epoch 45/100: loss=13.2326\n",
      "Epoch 46/100: loss=13.7638\n",
      "Epoch 47/100: loss=14.0406\n",
      "Epoch 48/100: loss=13.5416\n",
      "Epoch 49/100: loss=14.2893\n",
      "Epoch 50/100: loss=12.9522\n",
      "Epoch 51/100: loss=14.2236\n",
      "Epoch 52/100: loss=13.8046\n",
      "Epoch 53/100: loss=13.8249\n",
      "Epoch 54/100: loss=13.8445\n",
      "Epoch 55/100: loss=13.4690\n",
      "Epoch 56/100: loss=14.1812\n",
      "Epoch 57/100: loss=13.7156\n",
      "Epoch 58/100: loss=13.5406\n",
      "Epoch 59/100: loss=13.7123\n",
      "Epoch 60/100: loss=13.5961\n",
      "Epoch 61/100: loss=14.1524\n",
      "Epoch 62/100: loss=12.9717\n",
      "Epoch 63/100: loss=14.0762\n",
      "Epoch 64/100: loss=14.0667\n",
      "Epoch 65/100: loss=14.0756\n",
      "Epoch 66/100: loss=13.7511\n",
      "Epoch 67/100: loss=13.5518\n",
      "Epoch 68/100: loss=13.2103\n",
      "Epoch 69/100: loss=14.0120\n",
      "Epoch 70/100: loss=13.2868\n",
      "Epoch 71/100: loss=13.9678\n",
      "Epoch 72/100: loss=13.3788\n",
      "Epoch 73/100: loss=14.0472\n",
      "Epoch 74/100: loss=13.2157\n",
      "Epoch 75/100: loss=13.8109\n",
      "Epoch 76/100: loss=13.7366\n",
      "Epoch 77/100: loss=13.6142\n",
      "Epoch 78/100: loss=14.4922\n",
      "Epoch 79/100: loss=13.9805\n",
      "Epoch 80/100: loss=13.6715\n",
      "Epoch 81/100: loss=13.6478\n",
      "Epoch 82/100: loss=14.5647\n",
      "Epoch 83/100: loss=13.5958\n",
      "Epoch 84/100: loss=13.7030\n",
      "Epoch 85/100: loss=13.9200\n",
      "Epoch 86/100: loss=13.6831\n",
      "Epoch 87/100: loss=13.9233\n",
      "Epoch 88/100: loss=13.6113\n",
      "Epoch 89/100: loss=13.4339\n",
      "Epoch 90/100: loss=12.7883\n",
      "Epoch 91/100: loss=13.9006\n",
      "Epoch 92/100: loss=13.9013\n",
      "Epoch 93/100: loss=13.5238\n",
      "Epoch 94/100: loss=13.4796\n",
      "Epoch 95/100: loss=14.0243\n",
      "Epoch 96/100: loss=13.2010\n",
      "Epoch 97/100: loss=13.0511\n",
      "Epoch 98/100: loss=13.8856\n",
      "Epoch 99/100: loss=13.9415\n",
      "Epoch 100/100: loss=13.7568\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_longest_sentence:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.53125\n",
      "F1 Score: 0.5151515151515151\n",
      "\n",
      "*****************\n",
      "\n",
      "Epoch 1/100: loss=13.3236\n",
      "Epoch 2/100: loss=13.8197\n",
      "Epoch 3/100: loss=13.8948\n",
      "Epoch 4/100: loss=13.8233\n",
      "Epoch 5/100: loss=13.5170\n",
      "Epoch 6/100: loss=14.1657\n",
      "Epoch 7/100: loss=14.1808\n",
      "Epoch 8/100: loss=13.1682\n",
      "Epoch 9/100: loss=13.8793\n",
      "Epoch 10/100: loss=14.1376\n",
      "Epoch 11/100: loss=13.3337\n",
      "Epoch 12/100: loss=14.0433\n",
      "Epoch 13/100: loss=13.5301\n",
      "Epoch 14/100: loss=13.6797\n",
      "Epoch 15/100: loss=12.9292\n",
      "Epoch 16/100: loss=14.0263\n",
      "Epoch 17/100: loss=13.2910\n",
      "Epoch 18/100: loss=13.4033\n",
      "Epoch 19/100: loss=13.7354\n",
      "Epoch 20/100: loss=14.0165\n",
      "Epoch 21/100: loss=14.2064\n",
      "Epoch 22/100: loss=13.5038\n",
      "Epoch 23/100: loss=13.4086\n",
      "Epoch 24/100: loss=13.8001\n",
      "Epoch 25/100: loss=13.8398\n",
      "Epoch 26/100: loss=13.8614\n",
      "Epoch 27/100: loss=13.2480\n",
      "Epoch 28/100: loss=14.2215\n",
      "Epoch 29/100: loss=14.2060\n",
      "Epoch 30/100: loss=13.9022\n",
      "Epoch 31/100: loss=14.2951\n",
      "Epoch 32/100: loss=13.3657\n",
      "Epoch 33/100: loss=13.6456\n",
      "Epoch 34/100: loss=13.2289\n",
      "Epoch 35/100: loss=13.8592\n",
      "Epoch 36/100: loss=13.9057\n",
      "Epoch 37/100: loss=13.7260\n",
      "Epoch 38/100: loss=13.4588\n",
      "Epoch 39/100: loss=13.1288\n",
      "Epoch 40/100: loss=13.8623\n",
      "Epoch 41/100: loss=13.8856\n",
      "Epoch 42/100: loss=14.2155\n",
      "Epoch 43/100: loss=13.7185\n",
      "Epoch 44/100: loss=13.1759\n",
      "Epoch 45/100: loss=14.1578\n",
      "Epoch 46/100: loss=13.2721\n",
      "Epoch 47/100: loss=13.7717\n",
      "Epoch 48/100: loss=14.2911\n",
      "Epoch 49/100: loss=13.8966\n",
      "Epoch 50/100: loss=13.3762\n",
      "Epoch 51/100: loss=13.7963\n",
      "Epoch 52/100: loss=13.5437\n",
      "Epoch 53/100: loss=13.5158\n",
      "Epoch 54/100: loss=13.5041\n",
      "Epoch 55/100: loss=13.7536\n",
      "Epoch 56/100: loss=14.1844\n",
      "Epoch 57/100: loss=13.4759\n",
      "Epoch 58/100: loss=14.4899\n",
      "Epoch 59/100: loss=13.7971\n",
      "Epoch 60/100: loss=13.3399\n",
      "Epoch 61/100: loss=13.7708\n",
      "Epoch 62/100: loss=14.1963\n",
      "Epoch 63/100: loss=13.7090\n",
      "Epoch 64/100: loss=13.7783\n",
      "Epoch 65/100: loss=13.7383\n",
      "Epoch 66/100: loss=13.7684\n",
      "Epoch 67/100: loss=13.7060\n",
      "Epoch 68/100: loss=13.6509\n",
      "Epoch 69/100: loss=13.5492\n",
      "Epoch 70/100: loss=14.0184\n",
      "Epoch 71/100: loss=13.9481\n",
      "Epoch 72/100: loss=13.6993\n",
      "Epoch 73/100: loss=13.7286\n",
      "Epoch 74/100: loss=13.7913\n",
      "Epoch 75/100: loss=13.5639\n",
      "Epoch 76/100: loss=13.9505\n",
      "Epoch 77/100: loss=14.1207\n",
      "Epoch 78/100: loss=13.9898\n",
      "Epoch 79/100: loss=14.0014\n",
      "Epoch 80/100: loss=13.7422\n",
      "Epoch 81/100: loss=14.0105\n",
      "Epoch 82/100: loss=14.3283\n",
      "Epoch 83/100: loss=14.1132\n",
      "Epoch 84/100: loss=13.1303\n",
      "Epoch 85/100: loss=13.5784\n",
      "Epoch 86/100: loss=13.2198\n",
      "Epoch 87/100: loss=14.5319\n",
      "Epoch 88/100: loss=13.8492\n",
      "Epoch 89/100: loss=13.7118\n",
      "Epoch 90/100: loss=14.1132\n",
      "Epoch 91/100: loss=13.5748\n",
      "Epoch 92/100: loss=13.7223\n",
      "Epoch 93/100: loss=13.5114\n",
      "Epoch 94/100: loss=13.4216\n",
      "Epoch 95/100: loss=13.3602\n",
      "Epoch 96/100: loss=13.2649\n",
      "Epoch 97/100: loss=13.2385\n",
      "Epoch 98/100: loss=14.2788\n",
      "Epoch 99/100: loss=13.5599\n",
      "Epoch 100/100: loss=14.0379\n",
      "\n",
      "*****************\n",
      "\n",
      "Ablation study for num_unique_words:\n",
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 0.445390625\n",
      "F1 Score: 0.47111808941409805\n",
      "\n",
      "*****************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the features to be used\n",
    "features = ['num_tokens', 'num_positive', 'num_negative', 'num_exclamation', 'num_question', 'num_last_paragraph', 'num_longest_sentence', 'num_unique_words' ]\n",
    "# Conduct the ablation study\n",
    "for feature in features:\n",
    "    # Create a new feature set with the current feature removed\n",
    "    ablation_features = [f for f in features]\n",
    "\n",
    "    # Train and evaluate the model with the current feature removed\n",
    "    log_reg_ablation = LogisticRegression(num_features=len(ablation_features))\n",
    "    weights, bias, loss = log_reg_ablation.train(train_x, train_y, num_epochs=100, batch_size=32)\n",
    "    accuracy, precision, recall, f1_score = log_reg_ablation.evaluate(test_x, test_y)\n",
    "\n",
    "    # Print the performance metrics for the current feature set\n",
    "    print('\\n*****************\\n')\n",
    "    print(f'Ablation study for {feature}:')\n",
    "    print('Accuracy:', accuracy)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('F1 Score:', f1_score)\n",
    "    print('\\n*****************\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
